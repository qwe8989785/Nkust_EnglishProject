# -*- coding: utf-8 -*-
"""SpacyPipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qqsoi4HN6hn8hiON1WDE_IfN-eWMpOL6

# Sentence Segmentation
Split paragraph into sentences
"""
import random
import spacy
import pandas as pd


def splitSent(sent):

  nlp = spacy.load("en_core_web_sm")
  #creat nlp document object
  doc = nlp(sent)
  data = []
  for item in doc.sents:
      data.append(item.text)
  return data
 



"""# Tokenization
Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units.  Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks.
"""
def tokenization(sent):
  nlp = spacy.load("en_core_web_sm")
  doc = nlp(sent)
  data = []
  for token in doc:
      data.append(token.text)
  return data


"""# Lemmatization
Lemmatization maps a word to its lemma (dictionary form). For instance, the word was is mapped to the word be.
"""
def lemmatization(sent):
  nlp = spacy.load("en_core_web_sm")
  doc = nlp(sent)
  data = []
  for token in doc:
    temp = {
      'text' : token.text,
      'lemma' : token.lemma_,
      'pos' : token.pos_,
      'tag' : token.tag_,
      'dep' : token.dep_,
      'shape' : token.shape_,
      'is_alpha' : token.is_alpha,
      'is_stop' : token.is_stop
    }
    data.append(temp)
  return data
    
"""# Extracting Words by POS Tags
Extracting words by part of speech
"""

def splitByPOS(sent):
  nlp = spacy.load("en_core_web_sm")
  #creat nlp document object
  doc = nlp(sent)
  data = {
    'nouns' : [],
    'adjectives' : [],
    'verbs' : [],
    'only_past' : []
  }
  for item in doc:
      if item.pos_ == 'NOUN':
          data['nouns'].append(item.text)
      elif item.pos_ == 'VERB':
          data['verbs'].append(item.text)
      elif item.pos_ == 'ADJ':
          data['adjectives'].append(item.text)
      elif item.tag_ == 'VBN':
          data['only_past'].append(item.text)
  #only the verbs in past participle form
  return data

def flatten_subtree(sent):
    return ''.join([w.text_with_ws for w in list(sent)]).strip()
"""# Extracting Phrase"""
def extractPhrase(sent):
  nlp = spacy.load("en_core_web_sm")
  #creat nlp document object
  doc = nlp(sent)
  #Split Sentences
  data = {
    'sents' : [],
    'flatten_subtree' : [],
    'nouns' : []
  }
  for item in doc.sents:
      data['sents'].append(item.text)
      data['flatten_subtree'].append(flatten_subtree(item.subtree))
  for item in doc.noun_chunks:
      data['nouns'].append(item.text)
  return data



"""# Extracting Noun Chunk, Print to the Console"""

def extractNounChunk(sent):
  nlp = spacy.load("en_core_web_sm")
  #creat nlp document object
  doc = nlp(sent)
  #Split Sentences
  data = {
    'sents' : [],
    'chunkText' : [],
    'chunkroot' : [],
    'rootDep' : [],
    'rootHead' : []
  }
  for item in doc.sents:
    data['sents'].append(item.text)
  for chunk in doc.noun_chunks:
     data['chunkText'].append(chunk.text)
     data['chunkroot'].append(chunk.root.text)
     data['rootDep'].append(chunk.root.dep_)
     data['rootHead'].append(chunk.root.head.text)
  return data


# -*- coding: utf-8 -*-
"""SpacyPipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qqsoi4HN6hn8hiON1WDE_IfN-eWMpOL6

# Sentence Segmentation
Split paragraph into sentences
"""
import random
import spacy
import pandas as pd
nlp = spacy.load('en')
#creat nlp document object
doc = nlp("All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.")
for item in doc.sents:
    print (item.text)

#Total number of sentences
sentences_as_list = list(doc.sents)
len(sentences_as_list)


#Randonly import one sentence from the list
random.choice(sentences_as_list)

"""# Tokenization
Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units.  Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks.
"""



nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")
for token in doc:
    print(token.text)

"""# Part Of Speech Tagging
Part of speech tagging assigns part of speech labels to tokens, such as whether they are verbs or nouns.
"""

doc = nlp("Next week I'll be in Madrid.")
for item in doc:
    print (item.text,"   ", item.pos_,"    ", item.tag_)
print([(token.text, token.tag_) for token in doc])

"""# Lemmatization
Lemmatization maps a word to its lemma (dictionary form). For instance, the word was is mapped to the word be.
"""


nlp = spacy.load('en')

doc = nlp(u"Apples and oranges are similar. Boots and hippos aren't.")
#For many token properties, such as part of speech and dependency relations, 
#  spaCy stores both integer and string attributes. 
#For example, for POS there is pos_ (string like "PUNCT" and "ADJ") 
#  and pos (integer values) attributes. The full list of token attributes is here.
#Token.lemma:Base form of the token, with no inflectional suffixes.(Integer)
#Token_lemma_:Base form of the token, with no inflectional suffixes.(Unicode)
for token in doc:
    print(token,"    ", token.lemma,"    ", token.lemma_)



nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)

"""# Extracting Words by POS Tags
Extracting words by part of speech
"""


nlp = spacy.load('en')
#creat nlp document object
doc = nlp("All human beings are born free and equal in dignity and rights.")
print (doc)
nouns = []
adjectives = []
for item in doc:
    if item.pos_ == 'NOUN':
        nouns.append(item.text)
print ("Nouns ==>", nouns)
verbs = []
for item in doc:
    if item.pos_ == 'VERB':
        verbs.append(item.text)
print("Verbs==>", verbs)
for item in doc:
    if item.pos_ == 'ADJ':
        adjectives.append(item.text)
print ("Adjectives ==>", adjectives)
only_past = []
for item in doc:
    if item.tag_ == 'VBN':
        only_past.append(item.text)
#only the verbs in past participle form
print("Parst prticiple verb==>", only_past)

"""# Extracting Phrase"""


nlp = spacy.load('en')
#creat nlp document object
doc = nlp("They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.")
#Split Sentences
for item in doc.sents:
    print ("sentences==>",item.text)
sentences_as_list = list(doc.sents)
print(len(sentences_as_list))
#import random
#Randonly import one sentence from the list
#random.choice(sentences_as_list)
#sentences_as_list = list(doc.sents)
#print ("Selected sentence==>",sentences_as_list)

#Noun Phrase
for item in doc.noun_chunks:
    print ("Noun Phrase==>", item.text)

def flatten_subtree(st):
    return ''.join([w.text_with_ws for w in list(st)]).strip()
    #for word in list(doc.sents)[2]:
    for word in list(doc.sents): 
      print ("Word:", word.text)
      print ("Flattened subtree: ", flatten_subtree(word.subtree))

"""# Extracting Noun Chunk, Print to the Console"""


nlp = spacy.load('en')
#creat nlp document object
doc = nlp("They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.")
#Split Sentences
for item in doc.sents:
    print ("sentences==>",item.text)
#print noun_chunk to console
print ('CHUNKS')
print ("chunkText      chunkroot      rootDep      rootHead")
for chunk in doc.noun_chunks:
   print (chunk.text,chunk.root.text, chunk.root.dep_,
                              chunk.root.head.text)

"""# Extracting Noun Chunk and save as HTML, and Pandas"""


nlp = spacy.load('en')
#creat nlp document object
doc = nlp("They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.")
#Split Sentences
for item in doc.sents:
    print ("sentences==>",item.text)
#print noun_chunk to console
print ('CHUNKS')
print ("chunkText      chunkroot      rootDep      rootHead")
for chunk in doc.noun_chunks:
  chunk_text = [chunk.text for chunk in doc.noun_chunks]
  chunk_root = [chunk.root.text for chunk in doc.noun_chunks]
  chunk_root_dep = [chunk.root.dep_ for chunk in doc.noun_chunks]
  chunk_root_head = [chunk.root.head.text for chunk in doc.noun_chunks]
df3 = pd.DataFrame(zip(chunk_text, chunk_root, chunk_root_head, chunk_root_head),
                   columns=['chunk_text','chunk_root','chunk_root_dep','chunk_root_head'])
df3.to_html('chunker.html')

"""# Dependency"""


nlp = spacy.load('en')
#creat nlp document object
doc = nlp("They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.")
#Split Sentences
for item in doc.sents:
    print ("sentences==>",item.text)
#print to console
print ('Dependency Relation')
print ('Text      Dep      Head Text    Head POS  Children')
for token in doc:
  print(token.text, token.dep_, token.head.text, token.head.pos_,
        [child for child in token.children])
  token_text = [token.text for token in doc]
  token_dep = [token.dep_ for token in doc]
  token_head_text = [token.head.text for token in doc]
  token_head_pos = [token.head.pos_ for token in doc]
  token_child=([child for child in token.children] for token in doc)
df1 = pd.DataFrame(zip(token_text, token_dep,token_head_text,token_head_pos,token_child),
                   columns=['token_text','token_dep','token_head','token_head_POS','token_child'])
df1.to_html('dependency.html')